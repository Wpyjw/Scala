| 管道符  用来连接多条命令的  命令1|命令2 用管道符连接的命令，命令 1 的正确输出作为命令 2 的操作对象

awk是一种可以处理数据、产生格式化报表的语言
awk “样式” 文件： 把符合样式的数据行显示出来。
awk { 操作 } 文件： 对每一行都执行{}中的操作。
awk " 样式 { 操作 }" 文件： 对符合样式的数据行，执行{}中的操作.

-i
若用-i参数，则bash是交互的。

-s
若用-s参数，则bash从标准输入中读入命令（在执行完-c带的命令之后。）直到输入exit。

$() ``都是用来作命令替换的 命令替换与变量替换差不多，都是用来重组命令行的，先完成引号里的命令行，然后将其结果替换出来，再重组成新的命令行。
$(命令) 返回该命令的结果

${} $var 变量替换
一般情况下，$var与${var}是没有区别的，但是用${ }会比较精确的界定变量名称的范围

# 是去掉左边(在键盘上 # 在 $ 之左边)
% 是去掉右边(在键盘上 % 在 $ 之右边)

bc命令是任意进度计算器语言，通常在linux下当计算器用
bc(选项)(参数)

$0 表示当前动行的命令名，一般用于shell 脚本中；
dirname 用于取指定路径所在的目录 ；如 dirname /usr/local/bin 结果为dirname /usr/local

source 使Shell读入指定的Shell程序文件并依次执行文件中的所有语句

#获取变量长度

S=${1:-2} if else


Spark部署
=========================================================================
Spark部署模式  local 单机模式  
              cluster 集群模式  stackalone  mesos   yarn 
              
                      yarn 
                          yarn cluster :这个就是生产环境常用的模式，所有的资源调度和计算都在集群环境上运行。
                          yarn client  :这个是说Spark Driver和ApplicationMaster进程均在本机运行，而计算任务在cluster上。
                          
                          
linux下如何执行文件


git流程
git已经配置好
1.开始->git->git bash
2.cd到要创建库的文件夹
3.git init 
4.登陆http://v9.git.n.xiaomi.com/  复制要下载的文件的地址
5.git clone wenjain dizhi

linux流程
直接git clone wenjian dizhi 
pwd 查看当前路径

mvn package 
sh -x lujingwenjian  日志

mvn pachage 完成打jar包的命令 构建项目软件包

RDD 弹性分布式数据集
不可变、可分区、里面的元素可并行计算的集合。
RDD 方法
map(func) 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成
flatMap(func) 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）
join(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD  相当于内连接（求交集
cogroup(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD

RDD的属性 1 一组分片

's'字符串插值器允许在处理字符串时直接使用变量

--class: 应用入口类（例如：org.apache.spark.examples.SparkPi)）

hadoop fs -ls列出指定目录下的文件及文件夹
-du 显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。

scala中=>
1.表示函数的返回类型
2.匿名函数
3.case语句
4.By-Name Parameters（传名参数）
闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量


Spark
Spark-Shell
val textFile = sc.textFile("README.md")
spark.RDD[String] = spark.MappedRDD@2ee9b6e3



RDDs支持2种类型的操作：
转换(transformations) 从已经存在的数据集中创建一个新的数据集；
  map 是一个转换操作，它将每一个数据集元素传递给一个函数并且返回一个新的 RDD。
动作(actions) 在数据集上进行计算之后返回一个值到驱动程序。
  reduce 是一个动作，它使用相同的函数来聚合 RDD 的所有元素，并且将最终的结果返回到驱动程序

样例类
使用了case关键字的类定义就是样例类，样例类是种特殊的类，经过优化以用于模式匹配。

scala Option(选项)类型用来表示一个值是可选的（有值或无值）
Option[T]是一个类型为T的可选值的容器：如果值存在，Option[T]就是一个Some[T],如果不存在，Option[T]就是对象None。

Scala
类名称 第一个字母应为大写。。如果使用多个单词来形成类的名称，则每个内部单词的第一个字母应该是大写。
方法名称 - 所有方法名称应以小写字母开头。如果使用多个单词形成方法的名称，则每个内部单词的第一个字母应为大写。

语法
var myVar:String="Foo"
val myVal:String="Foo"

多个赋值 如果代码块或方法返回一个元组，则可以将元组分配给一个val变量。
val (myVar1:Int,myVar2:String)=Pair(40,"Foo")
函数声明  def functionName ([list of parameters]) : [return type]
如果不使用等号和方法体，则隐式声明抽象方法

函数定义
def functionName ([list of parameters]): [return type]={
  function body
  return [expr]
}

def shadoubufanhui(): Unit{

}

调用方法
functionName(list of parameters)
[instance.]functionName( list of parameters )

匿名函数 (x: Int)=>x+1
函数文字，在运行时，函数文字被实例化成为函数值的对象
var inc= (x:Int)=>x+1
var x=inc(7)-1
var mul=(x:Int,y:Int)=>x*y
var userDir = () => { System.getProperty("user.dir") }

字符串
var greeting="Hello World";

var palindrome = "Dot saw I was Tod";
var len = palindrome.length();
声明数组变量
var z:Array[String]=new Array[String](3)
var z=new Array[String](3)
z(0)="3"
定义数组的方法 var z = Array("Maxsu", "Nancy", "Alen")

Scala元组 元组可以容纳不同类型的对象，但它们也是不可变的
val t=(1,"hello",Console)
val sum = t._1 + t._2 + t._3 + t._4
 Scala映射
声明不可变映射的示例声明
var A:Map[Char,Int]=Map()
var colors=Map("red"->"#FF00000","azure"->"#F0FFFF")

 curly braces form a block of code, which evaluates to the last line of code (all languages do this)；
 a function if desired can be generated with the block of code (follows rule 1)；
 curly braces can be omitted for one-line code except for a case clause (Scala choice)；
 parentheses can be omitted in function call with a single parameter (Scala choice)；
 almost never omit both。

 大括号{}用于代码块，计算结果是代码最后一行；
 大括号{}用于拥有代码块的函数；
 大括号{}在只有一行代码时可以省略，除了case语句（Scala适用）；
 小括号()在函数只有一个参数时可以省略（Scala适用）；
 几乎没有二者都省略的情况。

val tupleList = List[(String, String)]()
// doesn't compile, violates case clause requirement
val filtered = tupleList.takeWhile( case (s1, s2) => s1 == s2 )
// block of code as a partial function and parentheses omission,
// i.e. tupleList.takeWhile({ case (s1, s2) => s1 == s2 })
val filtered = tupleList.takeWhile{ case (s1, s2) => s1 == s2 }
 
// curly braces omission, i.e. List(1, 2, 3).reduceLeft({_+_})
List(1, 2, 3).reduceLeft(_+_)
// parentheses omission, i.e. List(1, 2, 3).reduceLeft({_+_})
List(1, 2, 3).reduceLeft{_+_}
// not both though it compiles, because meaning totally changes due to precedence
List(1, 2, 3).reduceLeft _+_ // res1: String => String = <function1>
 
// curly braces omission, i.e. List(1, 2, 3).foldLeft(0)({_ + _})
List(1, 2, 3).foldLeft(0)(_ + _)
// parentheses omission, i.e. List(1, 2, 3).foldLeft(0)({_ + _})
List(1, 2, 3).foldLeft(0){_ + _}
// block of code and parentheses omission
List(1, 2, 3).foldLeft {0} {_ + _}
// not both though it compiles, because meaning totally changes due to precedence
List(1, 2, 3).foldLeft(0) _ + _
// error: ';' expected but integer literal found.
List(1, 2, 3).foldLeft 0 (_ + _)

cogroup cogroup(otherDataset, [numTasks])是将输入数据集(K, V)和另外一个数据集(K, W)进行cogroup，得到一个格式为(K, Seq[V], Seq[W])的数据集。
两个RDD中具有重叠的KEY，并且这部分相同的KEY的数据需要进行互相处理（同时区分各自所属不同的RDD）
有点类似于groupByKey：但是goupByKey是在一个由（K,V）对组成的数据集上调用，返回一个（K,Seq[V])对的数据集。

join 当调用类型（K,V）和（K,W）的数据集时，返回（K,(V,W)）对的数据以及每个键的所有元素对
cogroup 当调用类型（K,V）和（K,W）的数据集时，返回（K,(Iterable<V>,Iterable<W>)）元组的数据集

/**
    * join（otherDataSet，[numTasks]）
    * 加入一个RDD，在一个（k，v）和（k，w）类型的dataSet上调用，返回一个（k，（v，w））的pair dataSet。
    */
  def join(): Unit ={
    val list1RDD = sc.parallelize(List((1, "华山派"), (2, "武当派"), (3, "明教"), (3, "崆峒派")))
    val list2RDD = sc.parallelize(List((1, 66), (2, 77), (3, 88)))
    list1RDD.join(list2RDD)
      .foreach(println)
  }

(1,(华山派,66))
(3,(明教,88))
(3,(崆峒派,88))
(2,(武当派,77))

/**
    * cogroup（otherDataSet，[numTasks]）
    * 对两个RDD中的KV元素，每个RDD中相同key中的元素分别聚合成一个集合。与reduceByKey不同的是针对
    * 两个RDD中相同的key的元素进行合并。
    *
    * 合并两个RDD，生成一个新的RDD。实例中包含两个Iterable值，第一个表示RDD1中相同值，第二个表示RDD2
    * 中相同值（key值），这个操作需要通过partitioner进行重新分区，因此需要执行一次shuffle操作。（
    * 若两个RDD在此之前进行过shuffle，则不需要）
    */
def cogroup(): Unit ={
    val list1RDD = sc.parallelize(List((1, "cat"), (2, "dog")))
    val list2RDD = sc.parallelize(List((1, "tiger"), (1, "elephant"), (3, "panda"), (3, "chicken")))
    val list3RDD = sc.parallelize(List((1, "duck"), (1, "lion"), (3, "bird"), (3, "fish"), (4, "flowers")))

    list1RDD.cogroup(list2RDD,list3RDD)
      .foreach(println)
  }

(4,(CompactBuffer(),CompactBuffer(),CompactBuffer(flowers)))
(1,(CompactBuffer(cat),CompactBuffer(tiger, elephant),CompactBuffer(duck, lion)))
(3,(CompactBuffer(),CompactBuffer(panda, chicken),CompactBuffer(bird, fish)))
(2,(CompactBuffer(dog),CompactBuffer(),CompactBuffer()))


git pull 从远程获取最新版本并merge 到本地
git fetch 相当于从远程获取最新到本地

git remote 用于管理主机名
git branch 的 -r 选项，可以用来查看远程分支，-a选项查看所有分支。

git status 查看当前分支的状态
使用git zai 本地新建一个分支后，需要做远程分支关联，如果没有关联，git会在下面的操作中提示你显示的添加关联。
git branch --set-upstream-to=origin/remote_branch  your_branch
mvn package 做完的项目要先打包
标准的推送步骤：
 git pull origin zhangwanpeng --allow-unrelated-histories
git branch --set-upstream-to=origin/zhangwanpeng zhangwanpeng
 git push origin zhangwanpeng

git pull origin zhangwanpeng
在Spark目录里使用下面的方式开始运行。

RDD  窄依赖 宽依赖
宽依赖：划分stage，每个stage包含一个或多个task任务

DAG 有向无环图

Spark中的每个Action对应一个Job

collect 将RDD类型的数据转化为数组，同时会从远程集群拉取数据到driver端

Spark 作业运行模式 
本地模式，Standalone模式，Spark on Mesos和Spark on Yarn

本地模式在一个jvm进程里面模拟多个角色，方便小数据量的本地调试。
Standalone模式为自己使用部署工具在机器上启动Spark Master和worker。
Spark on Mesos和Spark on Yarn模式是在Mesos和Yarn等集群调度框架运行Spark作业


- 本地模式：local/local-cluster
- standalone模式：spark://host:port
- spark on Meosos: mesos://host:port
- Spark on yarn: yarn

master 和 worker 是物理节点
driver 和 executor是进程

搭建spark集群的时候我们就已经设置好了master节点和worker节点，一个集群有多个master节点和多个worker节点。

master节点常驻master守护进程，负责管理worker节点，我们从master节点提交应用。

worker节点常驻worker守护进程，与master节点通信，并且管理executor进程。

PS：一台机器可以同时作为master和worker节点（举个例子：你有四台机器，你可以选择一台设置为master节点，然后剩下三台设为worker节点，也可以把四台都设为worker节点，这种情况下，有一个机器既是master节点又是worker节点）

driver 和 executor 进程
https://blog.csdn.net/hongmofang10/article/details/84587262

Spark 的API 在驱动程序里传递函数到集群上运行。
匿名函数
全局单例对象里的静态方法。
shuffle 描述数据从map task输出到reduce task 输入的这段过程。 shuffle是连接Map 和Reduce的桥梁，Map的输出要用到Reduce中必须经过shuffle这个环节。
shuffle的性能高低直接影响整个程序的性能和吞吐量。因为在分布式情况下，reduce task需要跨节点去拉取其它节点上的map task结果。这一过程将会产生网络资源消耗和内存，磁盘IO的消耗。
通常shuffle分为两部分：
Map阶段的数据准备和Reduce阶段的数据拷贝处理。
一般将在map端的Shuffle称之为Shuffle Write
在Reduce端的Shuffle称之为Shuffle Read.
https://blog.csdn.net/weixin_35602748/article/details/78656914
